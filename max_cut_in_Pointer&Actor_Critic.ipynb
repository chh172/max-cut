{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "max-cut-in-Pointer&Actor-Critic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNklGCRX4C8iwiRLf4x5PcW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chh172/max-cut/blob/main/max_cut_in_Pointer%26Actor_Critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from networkx import *\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import keras.backend as K\n",
        "# from keras.engine import InputSpec\n",
        "from tensorflow.keras.layers import InputSpec\n",
        "from keras.activations import tanh, softmax\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# copyright to https://github.com/keon/pointer-networks\n",
        "class Attention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "        Attention layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dimensions, name='attention'):\n",
        "        super(Attention, self).__init__(name=name, trainable=True)\n",
        "        self.W1 = keras.layers.Dense(hidden_dimensions, use_bias=False)\n",
        "        self.W2 = keras.layers.Dense(hidden_dimensions, use_bias=False)\n",
        "        self.V = keras.layers.Dense(1, use_bias=False)\n",
        "\n",
        "    def call(self, encoder_outputs, dec_output, mask=None):\n",
        "\n",
        "        w1_e = self.W1(encoder_outputs)\n",
        "        w2_d = self.W2(dec_output)\n",
        "        tanh_output = tanh(w1_e + w2_d)\n",
        "        v_dot_tanh = self.V(tanh_output)\n",
        "        #print(v_dot_tanh.shape)\n",
        "        if mask is not None:\n",
        "            v_dot_tanh += (mask * -1e9)\n",
        "        attention_weights = softmax(v_dot_tanh, axis=1)\n",
        "        att_shape = K.shape(attention_weights)\n",
        "        return K.reshape(attention_weights, (att_shape[0], att_shape[1]))\n",
        "\n",
        "\n",
        "class Decoder(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "        Decoder class for PointerLayer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dimensions):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm = keras.layers.LSTM(\n",
        "            hidden_dimensions, return_sequences=False, return_state=True)\n",
        "\n",
        "    def call(self, x, hidden_states):\n",
        "        dec_output, state_h, state_c = self.lstm(\n",
        "            x, initial_state=hidden_states)\n",
        "        return dec_output, [state_h, state_c]\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        return self.lstm.get_initial_state(inputs)\n",
        "\n",
        "    def process_inputs(self, x_input, initial_states, constants):\n",
        "        return self.lstm._process_inputs(x_input, initial_states, constants)\n",
        "\n",
        "\n",
        "class PointerLSTM(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "        PointerLSTM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dimensions, name='pointer', **kwargs):\n",
        "        super(PointerLSTM, self).__init__(**kwargs)\n",
        "        self.hidden_dimensions = hidden_dimensions\n",
        "        self.attention = Attention(hidden_dimensions)\n",
        "        self.decoder = Decoder(hidden_dimensions)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(PointerLSTM, self).build(input_shape)\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "\n",
        "    def call(self, x, training=None, mask=None, states=None):\n",
        "        \"\"\"\n",
        "        :param Tensor x: Should be the output of the decoder\n",
        "        :param Tensor states: last state of the decoder\n",
        "        :param Tensor mask: The mask to apply\n",
        "        :return: Pointers probabilities\n",
        "        \"\"\"\n",
        "\n",
        "        input_shape = self.input_spec[0].shape\n",
        "        en_seq = x\n",
        "        x_input = x[:, input_shape[1] - 1, :]\n",
        "        x_input = K.repeat(x_input, input_shape[1])\n",
        "        if states:\n",
        "            initial_states = states\n",
        "        else:\n",
        "            initial_states = self.decoder.get_initial_state(x_input)\n",
        "\n",
        "        constants = []\n",
        "        preprocessed_input, _, constants = self.decoder.process_inputs(\n",
        "            x_input, initial_states, constants)\n",
        "        constants.append(en_seq)\n",
        "        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n",
        "                                             initial_states,\n",
        "                                             go_backwards=self.decoder.lstm.go_backwards,\n",
        "                                             constants=constants,\n",
        "                                             input_length=input_shape[1])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def step(self, x_input, states):\n",
        "        x_input = K.expand_dims(x_input,1)\n",
        "        input_shape = self.input_spec[0].shape\n",
        "        en_seq = states[-1]\n",
        "        _, [h, c] = self.decoder(x_input, states[:-1])\n",
        "        dec_seq = K.repeat(h, input_shape[1])\n",
        "        probs = self.attention(dec_seq, en_seq)\n",
        "        return probs, [h, c]\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        # output shape is not affected by the attention component\n",
        "        return (input_shape[0], input_shape[1], input_shape[1])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[1])"
      ],
      "metadata": {
        "id": "FbGFhCwjtHY9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "from networkx import *\n",
        "n = 6\n",
        "p = 0.5\n",
        "g = erdos_renyi_graph(n, p)\n",
        "print(g. nodes)\n",
        "# [0, 1, 2, 3, 4, 5] \n",
        "print(g. edges)\n",
        "# [(0, 1), (0, 2), (0, 4), (1, 2), (1, 5), (3, 4), (4, 5)]\n",
        "vertices = 5\n",
        "A = adjacency_matrix(erdos_renyi_graph(vertices, 0.5)).todense()\n",
        "print(A)\n",
        "B = tf.concat([A,tf.zeros([1,5])],0)\n",
        "print(B)\n",
        "C = tf.concat([B,tf.zeros([6,1])],1)\n",
        "print(C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f-O7ecEb4Aj",
        "outputId": "41f010bc-67af-4d0f-8dd8-bf00c1e9ecc2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5]\n",
            "[(0, 1), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (3, 4), (3, 5), (4, 5)]\n",
            "[[0 0 0 0 1]\n",
            " [0 0 0 0 1]\n",
            " [0 0 0 1 1]\n",
            " [0 0 1 0 0]\n",
            " [1 1 1 0 0]]\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 1.]\n",
            " [0. 0. 1. 0. 0.]\n",
            " [1. 1. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]], shape=(6, 5), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]], shape=(6, 6), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Description: this function randomly generate a sequence of graphs G(V,E)\n",
        "# where graphs are represented by their adjacency matrix A\n",
        "# and the distribution of V and E are all uniform.\n",
        "# It takes in 'number' of graphs, 'infimum' and 'supremum' of |V|. \n",
        "import numpy as np\n",
        "from networkx import *\n",
        "import random\n",
        "def graph_seq_generator(num, inf, sup):\n",
        "  atlas = []\n",
        "  for i in range(num):\n",
        "    A = adjacency_matrix(erdos_renyi_graph(np.random.randint(inf,sup), 0.5))\n",
        "    #print(A.todense())\n",
        "    atlas.append(A.todense())\n",
        "  return atlas    \n",
        "\n"
      ],
      "metadata": {
        "id": "HXAwLP_Hh6S3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Description: this function tensor-ize the atlas, preparing for LSTM\n",
        "def input_generator(atlas):\n",
        "  return tf.cast(np.array(random.sample(atlas,1)),tf.float32)"
      ],
      "metadata": {
        "id": "9zlJq0DRyAez"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Description: this function randomly generate a random graph G(V,E)\n",
        "# where graph is represented by their adjacency matrix A, plus 'Split'\n",
        "\n",
        "def graph_generator(vertices):\n",
        "  A = adjacency_matrix(erdos_renyi_graph(vertices, 0.5)).todense()\n",
        "  B = tf.concat([A,tf.zeros([1,vertices])],0)\n",
        "  C = tf.concat([B,tf.zeros([vertices+1,1])],1)\n",
        "  return tf.cast(np.array([C]),tf.float32)\n",
        "\n",
        "print(graph_generator(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfxkb4KMs52M",
        "outputId": "3368a0e7-2499-46ea-bccb-bbc1d939e08a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0. 1. 0. 0. 1. 0.]\n",
            "  [1. 0. 1. 1. 1. 0.]\n",
            "  [0. 1. 0. 0. 1. 0.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  [1. 1. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0.]]], shape=(1, 6, 6), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unit testing\n",
        "# atlas = graph_seq_generator(4,1,5)\n",
        "#print(atlas[1])\n",
        "#inp = Input(atlas[1])\n",
        "#print(inp)\n",
        "#for i in range(3):\n",
        "  #print(g[i].nodes)\n",
        "  #\n",
        "  #print(g[i].edges)\n",
        "  #\n",
        "  #print(len(g[i].nodes))\n",
        "#g = random.sample(atlas,2)\n",
        "#print(g[1])\n",
        "#input = input_generator(atlas)\n",
        "#input_2 = tf.cast(np.array([atlas[1]]),tf.float32)\n",
        "#print(input)\n",
        "#print(input_2)\n",
        "#print(input.shape[0])\n",
        "#print(input.shape[1])\n",
        "#print(input.shape[2])\n",
        "#print(input.shape)\n",
        "#print(input.shape)\n",
        "# x = Input(shape=(32,))\n",
        "#x_1 = Input(input)\n",
        "# print(x)\n",
        "# print(graph_generator(5))\n",
        "# print(tf.keras.Input(shape=(10,None,5),tensor=graph_generator(5)))\n",
        "def increment(x):\n",
        "  x = x +1\n",
        "  return\n",
        "x = 1\n",
        "increment(x)\n",
        "print(x)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-bmxoRsJmcc",
        "outputId": "b8a72eb5-0815-49ad-90e9-b336d1c48a41"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# actor-critic training\n",
        "# starting state: (0,0,...,0)\n",
        "# action space = {action: flip a single bit in state} U {ternimate}\n",
        "def actor_critic_algo(actor,critic,graph):\n",
        "  # randomly init 'phi', 'theta'\n",
        "  # do until theta converges (loop for every graph)\n",
        "    # init starting state\n",
        "    # \\lambda = 1\n",
        "    # do until reach terminal state\n",
        "      # In state 's', select action 'a' given by actor\n",
        "      # perform 'a' and collect reward r and new state (terminal state)\n",
        "      # update delta\n",
        "      # update value function parameter 'phi'\n",
        "      # update policy parameter 'theta'\n",
        "      # update discount \n",
        "      # update state\n",
        "  return\n",
        "\n",
        "def delta_update(critic,reward):\n",
        "  return \n",
        "\n",
        "def value_f_update(critic, delta ,phi, beta):\n",
        "  return phi+ beta*delta*value_grad(critic,phi)\n",
        "\n",
        "def policy_update(actor,alpha,lamb,delta,theta):\n",
        "  return theta + alpha*lamb*delta*policy_grad(actor,theta)\n",
        "#  \n",
        "def policy_grad(actor,theta):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(theta)\n",
        "    y = tf.math.log(actor(theta))\n",
        "  return tape.gradient(y,theta) \n",
        "def value_grad(critic,phi):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(phi)\n",
        "    y = critic(phi)\n",
        "  return tape.gradient(y, phi) "
      ],
      "metadata": {
        "id": "UuBJub5Wd6dR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# training main in way 2\n",
        "\n",
        "hidden_size = 128\n",
        "vertices_size = 5\n",
        "\n",
        "# input is a [A], where A is the adjacency matrix, which is a square matrix\n",
        "# sequence length shall match the number of rows and feature_dim is number of cols\n",
        "# note the feature for each node is its adjacency relation (row vector in adjacency matrix)\n",
        "seq_len = vertices_size+1\n",
        "feature_dim = vertices_size+1\n",
        "\n",
        "encoder = LSTM(hidden_size,return_sequences = True, name=\"encoder\",return_state=True)\n",
        "actor_decoder = PointerLSTM( hidden_size, name=\"actor_decoder\")\n",
        "critic_decoder = LSTM(hidden_size,name=\"critic_decoder\")\n",
        "\n",
        "inputs = keras.layers.Input(shape=(seq_len, feature_dim)) \n",
        "encoder_o, state_h, state_c = encoder(inputs)\n",
        "policy = actor_decoder(encoder_o,states=[state_h, state_c])\n",
        "scores = critic_decoder(encoder_o)\n",
        "\n",
        "\n",
        "\n",
        "#actor = tf.keras.Model(inputs=inputs, outputs=policy)\n",
        "\n",
        "#critic = tf.keras.Model(inputs=inputs, outputs=scores)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=[policy,scores])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              loss=tf.keras.losses.Huber(),\n",
        "              metrics=['accuracy'])\n",
        "graph = graph_generator(5)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(graph)\n",
        "  y_p, y_c= model(graph)\n",
        "\n",
        "grad = tape.gradient(y_p,graph)\n",
        "print(grad)\n",
        "# for i in range(5):\n",
        "graph = graph_generator(vertices_size)\n",
        "print(graph)\n",
        "encoder_outputs, state_h, state_c = encoder(graph)\n",
        "#print(encoder_outputs)\n",
        "print(\"hello world\")\n",
        "policy = actor_decoder(encoder_outputs)\n",
        "print(policy)\n",
        "scores = critic_decoder(encoder_outputs)\n",
        "print(\"hello world\")\n",
        "#print(scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEWMHPkVeFnT",
        "outputId": "d663ee24-4133-4fdf-b440-8be95ff799cf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[ 2.5537046e-09  1.0978521e-09 -8.9404284e-10 -9.8692543e-10\n",
            "    3.5759673e-11  1.1205442e-09]\n",
            "  [ 2.1216673e-09  8.6403529e-10 -8.8642960e-10 -1.1226853e-09\n",
            "    2.9474451e-11  1.1536196e-09]\n",
            "  [ 1.9193482e-09  7.6459561e-10 -1.0748350e-09 -6.4104588e-10\n",
            "    3.2238695e-10  1.1712280e-09]\n",
            "  [ 1.2285043e-09  5.6652194e-10 -1.3500033e-09 -3.4017464e-10\n",
            "    3.2190764e-10  1.2637844e-09]\n",
            "  [ 3.5845310e-10  4.9499321e-10 -1.9038588e-09  3.3399586e-10\n",
            "    5.7116345e-10  1.2253848e-09]\n",
            "  [-5.7944033e-10  5.5333099e-10 -2.3682127e-09  1.2020662e-09\n",
            "    1.0800154e-09  1.2131864e-09]]], shape=(1, 6, 6), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[0. 0. 1. 1. 1. 0.]\n",
            "  [0. 0. 0. 1. 0. 0.]\n",
            "  [1. 0. 0. 1. 0. 0.]\n",
            "  [1. 1. 1. 0. 1. 0.]\n",
            "  [1. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0.]]], shape=(1, 6, 6), dtype=float32)\n",
            "hello world\n",
            "tf.Tensor(\n",
            "[[[0.1723024  0.1740142  0.17062278 0.16250569 0.16030987 0.16024496]\n",
            "  [0.17230417 0.17401507 0.17062151 0.16250682 0.16030775 0.16024469]\n",
            "  [0.17230508 0.17401531 0.17062028 0.1625081  0.16030653 0.16024467]\n",
            "  [0.17230552 0.17401525 0.17061922 0.16250937 0.1603059  0.16024476]\n",
            "  [0.17230567 0.17401505 0.17061831 0.16251053 0.16030557 0.16024487]\n",
            "  [0.17230566 0.17401479 0.1706176  0.16251153 0.16030544 0.16024497]]], shape=(1, 6, 6), dtype=float32)\n",
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "metadata": {
        "id": "XViDljAey-BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training main in way 2\n",
        "\n",
        "hidden_size = 4\n",
        "vertices_size = 2\n",
        "\n",
        "# input is a [A], where A is the adjacency matrix, which is a square matrix\n",
        "# sequence length shall match the number of rows and feature_dim is number of cols\n",
        "# note the feature for each node is its adjacency relation (row vector in adjacency matrix)\n",
        "seq_len = vertices_size+1\n",
        "feature_dim = vertices_size+1\n",
        "\n",
        "encoder = LSTM(hidden_size,return_sequences = True, name=\"encoder\",return_state=True)\n",
        "actor_decoder = PointerLSTM( hidden_size, name=\"actor_decoder\")\n",
        "critic_decoder = LSTM(hidden_size,name=\"critic_decoder\")\n",
        "\n",
        "inputs = keras.layers.Input(shape=(seq_len, feature_dim)) \n",
        "encoder_o, state_h, state_c = encoder(inputs)\n",
        "policy = actor_decoder(encoder_o,states=[state_h, state_c])\n",
        "scores = critic_decoder(encoder_o)\n",
        "\n",
        "\n",
        "\n",
        "#actor = tf.keras.Model(inputs=inputs, outputs=policy)\n",
        "\n",
        "#critic = tf.keras.Model(inputs=inputs, outputs=scores)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=[policy,scores])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "              loss=tf.keras.losses.Huber(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "graph = graph_generator(vertices_size)\n",
        "print(graph)\n",
        "encoder_outputs, state_h, state_c = encoder(graph)\n",
        "print(encoder_outputs)\n",
        "print(\"hello world\")\n",
        "policy = actor_decoder(encoder_outputs)\n",
        "print(policy)\n",
        "scores = critic_decoder(encoder_outputs)\n",
        "print(\"hello world\")\n",
        "#print(scores)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBYsGYKkscVI",
        "outputId": "ac4e59fe-1903-4d69-a9dd-0dfd89c10f1b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0. 1. 0.]\n",
            "  [1. 0. 0.]\n",
            "  [0. 0. 0.]]], shape=(1, 3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[-0.0122238  -0.02103938  0.04114028  0.07900675]\n",
            "  [-0.09648272 -0.04284241 -0.07458316 -0.03733222]\n",
            "  [-0.0780471  -0.03441205 -0.0675756  -0.02653   ]]], shape=(1, 3, 4), dtype=float32)\n",
            "hello world\n",
            "tf.Tensor(\n",
            "[[[0.3655135  0.31468534 0.31980118]\n",
            "  [0.36550426 0.31468827 0.31980747]\n",
            "  [0.3654951  0.31469196 0.31981292]]], shape=(1, 3, 3), dtype=float32)\n",
            "hello world\n",
            "Model: \"model_21\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_22 (InputLayer)          [(None, 3, 3)]       0           []                               \n",
            "                                                                                                  \n",
            " encoder (LSTM)                 [(None, 3, 4),       128         ['input_22[0][0]']               \n",
            "                                 (None, 4),                                                       \n",
            "                                 (None, 4)]                                                       \n",
            "                                                                                                  \n",
            " pointer_lstm_21 (PointerLSTM)  (None, 3, 3)         180         ['encoder[0][0]',                \n",
            "                                                                  'encoder[0][1]',                \n",
            "                                                                  'encoder[0][2]']                \n",
            "                                                                                                  \n",
            " critic_decoder (LSTM)          (None, 4)            144         ['encoder[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 452\n",
            "Trainable params: 452\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}