{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "max-cut-in-Pointer&Actor-Critic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJkd2W6OtgJJp7zonx1w2l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chh172/max-cut/blob/main/max_cut_in_Pointer%26Actor_Critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XViDljAey-BG",
        "outputId": "e6a94061-1453-4281-e1b8-ca9d6a118f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.8.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random \n",
        "import tensorflow as tf\n",
        "from networkx import *\n",
        "n = 6\n",
        "p = 0.5\n",
        "g = erdos_renyi_graph(n, p)\n",
        "print(g. nodes)\n",
        "# [0, 1, 2, 3, 4, 5] \n",
        "print(g. edges)\n",
        "# [(0, 1), (0, 2), (0, 4), (1, 2), (1, 5), (3, 4), (4, 5)]\n",
        "vertices = 5\n",
        "A = adjacency_matrix(erdos_renyi_graph(vertices, 0.5)).todense()\n",
        "A = tf.cast(A,tf.float32)\n",
        "print(A)\n",
        "#B = tf.concat([A,tf.zeros([1,5])],0)\n",
        "#print(B)\n",
        "#C = tf.concat([B,tf.zeros([6,1])],1)\n",
        "#print(C)\n",
        "#D = adjacency_matrix(erdos_renyi_graph(vertices, 0.5)).todense()\n",
        "#D = tf.cast(D,tf.float32)\n",
        "#print(D)\n",
        "#print(A-D)\n",
        "#print(tf.norm(A-D))\n",
        "#print (1<2)\n",
        "print(A[0][4])"
      ],
      "metadata": {
        "id": "9f-O7ecEb4Aj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92df1cb5-2dc3-46d2-e370-718209d713f0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5]\n",
            "[(0, 3), (0, 5), (1, 2), (1, 5), (2, 5), (3, 4)]\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 1. 1.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1.]\n",
            " [1. 1. 0. 1. 0.]], shape=(5, 5), dtype=float32)\n",
            "tf.Tensor(1.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from networkx import *\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from tensorflow.keras.layers import InputSpec\n",
        "from keras.activations import tanh, softmax\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "\n",
        "# copyright to https://github.com/keon/pointer-networks\n",
        "class Attention(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "        Attention layer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dimensions, name='attention'):\n",
        "        super(Attention, self).__init__(name=name, trainable=True)\n",
        "        self.W1 = keras.layers.Dense(hidden_dimensions, use_bias=False)\n",
        "        self.W2 = keras.layers.Dense(hidden_dimensions, use_bias=False)\n",
        "        self.V = keras.layers.Dense(1, use_bias=False)\n",
        "\n",
        "    def call(self, encoder_outputs, dec_output, mask=None):\n",
        "\n",
        "        w1_e = self.W1(encoder_outputs)\n",
        "        w2_d = self.W2(dec_output)\n",
        "        tanh_output = tanh(w1_e + w2_d)\n",
        "        v_dot_tanh = self.V(tanh_output)\n",
        "        #print(v_dot_tanh.shape)\n",
        "        if mask is not None:\n",
        "            v_dot_tanh += (mask * -1e9)\n",
        "        attention_weights = softmax(v_dot_tanh, axis=1)\n",
        "        att_shape = K.shape(attention_weights)\n",
        "        return K.reshape(attention_weights, (att_shape[0], att_shape[1]))\n",
        "\n",
        "\n",
        "class Decoder(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "        Decoder class for PointerLayer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dimensions):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm = keras.layers.LSTM(\n",
        "            hidden_dimensions, return_sequences=False, return_state=True)\n",
        "\n",
        "    def call(self, x, hidden_states):\n",
        "        dec_output, state_h, state_c = self.lstm(\n",
        "            x, initial_state=hidden_states)\n",
        "        return dec_output, [state_h, state_c]\n",
        "\n",
        "    def get_initial_state(self, inputs):\n",
        "        return self.lstm.get_initial_state(inputs)\n",
        "\n",
        "    def process_inputs(self, x_input, initial_states, constants):\n",
        "        return self.lstm._process_inputs(x_input, initial_states, constants)\n",
        "\n",
        "\n",
        "class PointerLSTM(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "        PointerLSTM\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dimensions, name='pointer', **kwargs):\n",
        "        super(PointerLSTM, self).__init__(**kwargs)\n",
        "        self.hidden_dimensions = hidden_dimensions\n",
        "        self.attention = Attention(hidden_dimensions)\n",
        "        self.decoder = Decoder(hidden_dimensions)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(PointerLSTM, self).build(input_shape)\n",
        "        self.input_spec = [InputSpec(shape=input_shape)]\n",
        "\n",
        "    def call(self, x, training=None, mask=None, states=None):\n",
        "        \"\"\"\n",
        "        :param Tensor x: Should be the output of the decoder\n",
        "        :param Tensor states: last state of the decoder\n",
        "        :param Tensor mask: The mask to apply\n",
        "        :return: Pointers probabilities\n",
        "        \"\"\"\n",
        "\n",
        "        input_shape = self.input_spec[0].shape\n",
        "        en_seq = x\n",
        "        x_input = x[:, input_shape[1] - 1, :]\n",
        "        x_input = K.repeat(x_input, input_shape[1])\n",
        "        if states:\n",
        "            initial_states = states\n",
        "        else:\n",
        "            initial_states = self.decoder.get_initial_state(x_input)\n",
        "\n",
        "        constants = []\n",
        "        preprocessed_input, _, constants = self.decoder.process_inputs(\n",
        "            x_input, initial_states, constants)\n",
        "        constants.append(en_seq)\n",
        "        last_output, outputs, states = K.rnn(self.step, preprocessed_input,\n",
        "                                             initial_states,\n",
        "                                             go_backwards=self.decoder.lstm.go_backwards,\n",
        "                                             constants=constants,\n",
        "                                             input_length=input_shape[1])\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def step(self, x_input, states):\n",
        "        x_input = K.expand_dims(x_input,1)\n",
        "        input_shape = self.input_spec[0].shape\n",
        "        en_seq = states[-1]\n",
        "        _, [h, c] = self.decoder(x_input, states[:-1])\n",
        "        dec_seq = K.repeat(h, input_shape[1])\n",
        "        probs = self.attention(dec_seq, en_seq)\n",
        "        return probs, [h, c]\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        # output shape is not affected by the attention component\n",
        "        return (input_shape[0], input_shape[1], input_shape[1])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[1])"
      ],
      "metadata": {
        "id": "FbGFhCwjtHY9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Description: this function randomly generate a sequence of graphs G(V,E)\n",
        "# where graphs are represented by their adjacency matrix A\n",
        "# and the distribution of V and E are all uniform.\n",
        "\n",
        "import numpy as np\n",
        "from networkx import *\n",
        "import random\n",
        "def graph_batch_generator(batch, vertices):\n",
        "  atlas = []\n",
        "  for i in range(batch):\n",
        "    A = adjacency_matrix(erdos_renyi_graph(vertices, 0.5)).todense()\n",
        "    B = tf.concat([A,tf.zeros([1,vertices])],0)\n",
        "    C = tf.concat([B,tf.zeros([vertices+1,1])],1)\n",
        "    atlas.append(C)\n",
        "  return tf.cast(np.array(atlas),tf.float32)   "
      ],
      "metadata": {
        "id": "HXAwLP_Hh6S3"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Description: this function randomly generate a random graph G(V,E)\n",
        "# where graph is represented by their adjacency matrix A, plus 'Split'\n",
        "\n",
        "def graph_generator(vertices):\n",
        "  A = adjacency_matrix(erdos_renyi_graph(vertices, 0.5)).todense()\n",
        "  B = tf.concat([A,tf.zeros([1,vertices])],0)\n",
        "  C = tf.concat([B,tf.zeros([vertices+1,1])],1)\n",
        "  return tf.cast(np.array([C]),tf.float32)\n",
        "\n",
        "print(graph_generator(5))"
      ],
      "metadata": {
        "id": "lfxkb4KMs52M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense\n",
        "\n",
        "# constants, alpha,beta learning rate\n",
        "disc_rate = 0.95\n",
        "alpha = 0.01\n",
        "beta = 0.01\n",
        "\n",
        "# Model construction\n",
        "def model_constr(vertices_size, hidden_size):\n",
        "\n",
        "  # input is a [A], where A is the adjacency matrix, which is a square matrix\n",
        "  # sequence length shall match the number of rows and feature_dim is number of cols\n",
        "  # note the feature for each node is its adjacency relation (row vector in adjacency matrix)\n",
        "  seq_len = vertices_size+1\n",
        "  feature_dim = vertices_size+1\n",
        "\n",
        "  encoder = LSTM(hidden_size, trainable=True,return_sequences = True, name=\"encoder\",return_state=True)\n",
        "  actor_decoder = PointerLSTM(hidden_size, trainable=True, name=\"actor_decoder\")\n",
        "  critic_decoder = LSTM(hidden_size, trainable=True,name=\"critic_decoder\")\n",
        "  value_f = Dense(1)\n",
        "\n",
        "  inputs = keras.layers.Input(shape=(seq_len, feature_dim)) \n",
        "  encoder_o, state_h, state_c = encoder(inputs)\n",
        "  policy = actor_decoder(encoder_o,states=[state_h, state_c])\n",
        "  decoder_o = critic_decoder(encoder_o)\n",
        "  scores = value_f(decoder_o)\n",
        "\n",
        "  actor = tf.keras.Model(inputs=inputs, outputs=policy)\n",
        "  critic = tf.keras.Model(inputs=inputs, outputs=scores)\n",
        "  actor.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "                loss=tf.keras.losses.Huber(),\n",
        "                metrics=['accuracy'])\n",
        "  critic.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "                loss=tf.keras.losses.Huber(),\n",
        "                metrics=['accuracy'])\n",
        "  return [actor,critic]\n"
      ],
      "metadata": {
        "id": "TBYsGYKkscVI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# MDP construction\n",
        "\n",
        "# 'State space, Action space, and Transition':\n",
        "# In our MDP, a graph as a whole is a state and each graph/non-terminal \n",
        "# state is associated with its characteristic action space A_s\n",
        "# i.e. the whole action action space A = U_s A_s partitioned by each state.\n",
        "# Moreover, we define a uniform terminal state s_t, i.e. p(s,a_s) := s_t for \n",
        "# any state s and its accompanied action a_s, where p is the transition function.\n",
        "\n",
        "# 'Reward':\n",
        "# Reward r of the action a_s from the state/graph s is defined to be \n",
        "# the cut-value produced by the cut of s under action a_s, \n",
        "# i.e. r(s,a_s) := cut_Value(s^{a_s}), \n",
        "# where s^{a_s} is the cut of s under action a_s. \n",
        "\n",
        "# actor-critic training\n",
        "def training(batch, vertices_size, hidden_size):\n",
        "  graphs = graph_batch_generator(batch,vertices_size)\n",
        "  actor, critic = model_constr(vertices_size, hidden_size)\n",
        "\n",
        "  # randomly init 'phi', 'theta' (already set)\n",
        "\n",
        "  lamb = 1\n",
        "    # do until theta converges (loop for every graph)\n",
        "  converge = False\n",
        "  for i in range(batch):\n",
        "    # randomly init 'phi', 'theta' (already set)\n",
        "    phi = critic.trainable_weights\n",
        "    theta = actor.trainable_weights\n",
        "    # init starting state\n",
        "    s = graphs[i]\n",
        "    # s virtually transitioned to terminal, but in fact stay the same till convergence\n",
        "    while not converge :\n",
        "      # In state 's', select action 'a' given by actor\n",
        "      a = actor.predict(graphs)[i]\n",
        "      # perform 'a' and collect reward r and new state (terminal state)\n",
        "      r = reward(s,a)        \n",
        "      # update delta\n",
        "      # print(critic.predict(graphs)[i].dtype)\n",
        "      delta = r - critic.predict(graphs)[i] \n",
        "      # update value function parameter 'phi'\n",
        "      phi = value_f_update(critic, delta ,phi, beta)   \n",
        "      critic.set_weights(phi)\n",
        "      # update policy parameter 'theta'\n",
        "      theta_p = policy_update(actor,alpha,lamb,delta,theta)\n",
        "      # update loop condition\n",
        "      converge = list_metric(theta, theta_p) < convergence_criteria\n",
        "      theta = theta_p\n",
        "      actor.set_weights(theta)\n",
        "      # update discount\n",
        "      lamb = lamb * disc_rate \n",
        "  return [actor, critic]\n"
      ],
      "metadata": {
        "id": "mkOA8nfYmsT6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: properly calculate reward\n",
        "def reward(state,action):\n",
        "  return 1.0\n",
        "\n",
        "def value_f_update(critic, delta ,phi, beta):\n",
        "  for i in range(len(phi)):\n",
        "    phi[i] = phi[i] + beta*delta*value_grad(critic,graphs)[i]\n",
        "  return phi\n",
        "\n",
        "\n",
        "def policy_update(actor,alpha,lamb,delta,theta):\n",
        "  for i in range(len(theta)):\n",
        "    theta[i] + alpha*lamb*delta*policy_grad(actor,graphs)[i]\n",
        "  return theta\n",
        "  \n",
        "def policy_grad(actor,graphs):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(graphs)\n",
        "    y = tf.math.log(actor(graphs))\n",
        "  return tape.gradient(y, actor.trainable_weights) \n",
        "def value_grad(critic,graphs):\n",
        "  with tf.GradientTape() as tape:\n",
        "    tape.watch(graphs)\n",
        "    y = critic(graphs)\n",
        "  return tape.gradient(y, critic.trainable_weights) \n",
        "# TODO: properly choose/define a metric between lists\n",
        "def list_metric(l_1,l_2):\n",
        "  dist = 0\n",
        "  for i in range(len(l_1)):\n",
        "    dist = dist + tf.norm(l_1[i] - l_2[i])\n",
        "  return dist\n"
      ],
      "metadata": {
        "id": "UuBJub5Wd6dR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constants, alpha,beta learning rate\n",
        "disc_rate = 0.95\n",
        "alpha = 0.01\n",
        "beta = 0.01\n",
        "convergence_criteria = 0.05\n",
        "batch = 2\n",
        "vertices_size = 3\n",
        "hidden_size = 4\n",
        "\n",
        "graphs = graph_batch_generator(batch,vertices_size)\n",
        "actor, critic = training(batch, vertices_size, hidden_size)\n",
        "\n",
        "\n",
        "\n",
        "#for i in range(len(value_grad(critic,graphs))):\n",
        "#  print( critic.trainable_weights[i].shape)\n",
        "  #print( 3.0 * critic.trainable_weights[i])\n",
        "#print(critic.trainable_weights.dtype == (value_grad(critic,graphs)[0]).dtype)\n",
        "phi = value_f_update(critic,1.0,critic.trainable_weights, 0.95)\n",
        "print(len(phi),type(phi))\n",
        "#print(tf.convert_to_tensor(critic.trainable_weights))\n",
        "#print(1/3)\n",
        "print(len(critic.trainable_weights),type(critic.trainable_weights))\n",
        "\n",
        "a = list_metric(critic.trainable_weights,phi)\n",
        "print(type(a))\n",
        "print(a)\n",
        "if a > 1/2:\n",
        "  print(1)\n",
        "\n",
        "# TODO: Before entering inference phase, set trainable to False\n",
        "actor.trainable = False\n",
        "critic.trainable = False\n",
        "print(actor.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNx61mXqsxni",
        "outputId": "5820828d-ce07-4225-f4f5-145644709e8a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 <class 'list'>\n",
            "8 <class 'list'>\n",
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            "tf.Tensor(3.7163935, shape=(), dtype=float32)\n",
            "1\n",
            "False\n"
          ]
        }
      ]
    }
  ]
}